import os
import json
import copy
from PIL import Image

import torch
from torch.utils.data import Dataset

from text_data.preprocess import SentPreProcessor


class SubsetDataset(Dataset):
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = targets
        self.transform = transform

    def __getitem__(self, index):

        path = self.data[index]
        label = self.targets[index]

        with open(path, 'rb') as f:
            sample = Image.open(f).convert('RGB')

        if self.transform is not None:
            sample = self.transform(sample)

        return sample, label

    def __len__(self):
        return len(self.targets)


class TextTokenMemoryDataset(Dataset):
    def __init__(self, text_tokens: list, num_sents: list):
        self.data = torch.cat(text_tokens)
        self.targets = []

        targets = [[idx]*nsents for idx, nsents in enumerate(num_sents)]
        for t in targets: self.targets += t

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, index):
        sample = self.data[index]
        label = self.targets[index]

        return sample, label


"""
ClassDisjointAbstractDataset

Parameters
----------
    * root : str
        root directory of ImageNet100 dataset
    * train : bool
        True if load train dataset. False otherwise (validation, test dataset)
    * sub_dirs : list ['train.X1', 'train.X2', 'train.X3', 'train.X4', 'val.X']
        sub directories that you want to include in
        if you want to make validation dataset, just use ['val.X']
    * label_file : str
        label file of dataset
        if you remain it empty, it will make for you automatically
        But, if you make a validation(or test) dataset, you must specify the label file what you use for it
        Just use same label file of train dataset that has same setting with your validation dataset
    * label_mapping_file : str
        it is "labels.txt" file. you can find it in this link
        https://github.com/ChangyaoTian/VL-LTR/releases/download/text-corpus/imagenet.zip
    * wiki_dir : str
        it is "wiki" directory. you can find it in upper link
    * max_classes : None, int
        maximum number of classes.
        you can control the number of classes with this argument.
        It will cut out classes like this. [0 ~ C] -> [0 ~ max_classes-1]
    * max_samples : None, int
        maximum number of samples of classes.
        you can control the number of smaples of classes with this argument.
    * transform
        just transform

Attribute (may someone needed....)
-------
    * loaded_idxs : 1 x max_classes
        The indecies that dataset actually consist of. It look like ['n01440764', 'n01484850', ....]
    * num_classes
        The number of classes
    * num_samples
        number of samples of classes.
    * text_tokens : list[torch.tensor x num_classes]
        tokens of wikipedia descriptions generated by clip.simple_tokenizer
        Each tokens has shape like [#_sentences x (context length + 2)] (2 for start, end token)
    * num_sents
        number of sentences of each classes. same as #_sentences
"""
class ClassDisjointAbstractDataset(Dataset):
    def __init__(self, root, sub_dirs = [], label_file = '', label_mapping_file = 'labels.txt', wiki_dir = 'wiki', max_samples = None, transform=None, is_shot=False, len_shot=16):
        self.root = root

        self.transform = transform
        self.len_shot = len_shot # query : [0 ~ (-len_shot - 1 or max_samples)], memory : [len_shot ~ to the last elem]
        self.sub_dirs = sub_dirs # choose in ['train.X1', 'train.X2', 'train.X3', 'train.X4', 'val.X']
        self.sub_idxs = [sorted(os.listdir(os.path.join(self.root, subdir))) for subdir in self.sub_dirs]

        idxs_cls = self._label_generator(root, label_file)
        loaded_idxs = idxs_cls.keys()
        self.loaded_idxs = sorted(loaded_idxs, key = lambda item: idxs_cls[item]) # sort idxs with 0 ~ (# of classes - 1) order
        self.num_classes = len(self.loaded_idxs)

        self.img_path = []
        self.targets = []

        num_samples_count = [0] * self.num_classes

        for i, idxs in enumerate(self.sub_idxs):  # iterate over classes
            for idx in idxs:  # iterate over instances
                if idx not in self.loaded_idxs: continue

                imgdirs = sorted(os.listdir(os.path.join(root, self.sub_dirs[i], idx)))
                if is_shot:
                    num_samples_i = self.len_shot if self.len_shot else len(imgdirs)
                    imgdirs = imgdirs[-num_samples_i:]
                    # imgdirs = imgdirs[200:]  # full
                else:
                    num_samples_i = min(max_samples, len(imgdirs) - self.len_shot) if max_samples else len(imgdirs) - self.len_shot
                    # num_samples_i = max_samples if max_samples else len(imgdirs) - self.len_shot  # full
                    imgdirs = imgdirs[:num_samples_i]

                for imgdir in imgdirs:
                    target = idxs_cls[idx]
                    if num_samples_count[target] >= num_samples_i: continue

                    self.img_path.append(os.path.join(root, self.sub_dirs[i], idx, imgdir))
                    self.targets.append(target)

                    num_samples_count[target] += 1

        assert self.num_classes == max(self.targets) + 1

        # sentence token generator
        self.text_tokens = self._make_sentence_tokens(label_mapping_file, wiki_dir)
        self.num_sents = [token.shape[0] for token in self.text_tokens]

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, index):

        path = self.img_path[index]
        label = self.targets[index]

        with open(path, 'rb') as f:
            sample = Image.open(f).convert('RGB')

        if self.transform is not None:
            sample = self.transform(sample)

        return sample, label

    def _label_generator(self, root, txt):
        if txt != '' and os.path.exists(os.path.join(root, txt)):
            print(f'Label file exist : {os.path.join(root, txt)}')
            with open(os.path.join(root, txt), 'r') as jsonfile:
                idx_classes = json.load(jsonfile)
        else:
            txt = 'label.json' if txt == '' else txt
            print(f"No label file found, make new one on {os.path.join(root, txt)}")
            flatten_idxs = []
            for s in self.sub_idxs: flatten_idxs += s

            idx_classes = {}
            for i, idx in enumerate(flatten_idxs):
                idx_classes[idx] = i
            with open(os.path.join(root, txt), 'w') as jsonfile:
                json.dump(idx_classes, jsonfile, indent='')

        return idx_classes

    def _make_sentence_tokens(self, label_mapping_file, wiki_dir):
        preprocessor = SentPreProcessor(self.root, self.loaded_idxs, label_mapping_file, wiki_dir, context_length=75)
        return preprocessor.make_sentence_tokens()


class CUBMemoryDataset(Dataset):
    def __init__(self, root, memory_dir='memory', label_file='standard_label.json', transform=None):
        self.transform = transform
        self.label_file = label_file

        self.memory_dir = os.path.join(root, memory_dir)
        with open(os.path.join(root, label_file)) as f:
            mapper = json.load(f)
        self.dir2target, self.txtlabels = self.mapper_refiner(mapper)

        folder_dirs = os.listdir(self.memory_dir)
        self.memory = self.get_all_files(folder_dirs)

    def mapper_refiner(self, mapper):
        txtlabels = []
        new_mapper = copy.deepcopy(mapper)

        for key1 in mapper.keys():
            # key2 add
            target = new_mapper[key1]
            key2 = key1.split('.')[1]
            new_mapper[key2] = target

            # get txtlabels
            key = ' '.join(key2.split('_'))
            txtlabels.append([target, key])
        ordered_txtlabels = ['' for _ in range(len(txtlabels))]
        for target, key in txtlabels:
            ordered_txtlabels[target] = key

        return new_mapper, txtlabels

    def get_all_files(self, folder_dirs):
        all_files = []

        for folder in folder_dirs:
            folder_key = '_'.join(folder.split())
            if folder_key not in self.dir2target.keys():
                continue
            folder_dir = os.path.join(self.memory_dir, folder)
            folder_files = os.listdir(folder_dir)
            target = self.dir2target[folder_key]

            for file in folder_files:
                if file.split('.')[-1] != 'jpg':
                    continue
                all_files.append([os.path.join(folder_dir, file), target])

        return all_files

    def __len__(self):
        return len(self.memory)

    def __getitem__(self, index):
        img_path, target = self.memory[index]
        img = Image.open(img_path).convert('RGB')
        if not (self.transform == None):
            img = self.transform(img)
        return img, target


class FoodMemoryDataset(CUBMemoryDataset):
     def mapper_refiner(self, mapper):
        txtlabels = []
        new_mapper = copy.deepcopy(mapper)

        for key1 in mapper.keys():
            # key2 add
            target = new_mapper[key1]
            key2 = key1
            new_mapper[key2] = target

            # get txtlabels
            key = ' '.join(key2.split('_'))
            txtlabels.append([target, key])
        ordered_txtlabels = ['' for _ in range(len(txtlabels))]
        for target, key in txtlabels:
            ordered_txtlabels[target] = key

        return new_mapper, txtlabels


class WebvisionMemoryDataset(Dataset):
    def __init__(self, root='webvision', label_file = '', synset2txtlabel={}, transform=None, len_memory=1000, webvisionsource='google'):
        self.transform = transform

        # n0xxxxxxx -> 'web' 0 ~ 999
        synset2webtarget = {}
        webtarget2synset = {}
        with open(os.path.join(root, 'info/synsets.txt')) as f:
            lines = f.readlines()
        for linenum, line in enumerate(lines):
            nxxxxxxxx = line.split()[0]
            synset2webtarget[nxxxxxxxx] = linenum
            webtarget2synset[linenum] = nxxxxxxxx

        with open(label_file) as f:
            idxs_cls = json.load(f)
        synset_set = idxs_cls.keys()  # [nxxxxxxxx, ..., nxxxxxxxx]
        num_classes = len(synset_set)

        # webvisionsource = {'google'/ 'flickr'}
        with open(os.path.join(root, f'info/train_filelist_{webvisionsource}.txt')) as f:
            lines = f.readlines()

        self.img_path = []
        self.targets = []
        num_samples_count = [0] * num_classes

        for line in lines:
            img, webtarget = line.split()
            webtarget = int(webtarget)

            if webtarget2synset[webtarget] in synset_set:
                # webvision is always memory
                synset = webtarget2synset[webtarget]
                target = idxs_cls[synset]
                # if num_samples_count[target] == 0:
                #     print(webtarget, '->', synset, '->', target)
                if num_samples_count[target] >= len_memory: continue
                self.img_path.append(os.path.join(root, img))
                self.targets.append(target)
                num_samples_count[target] += 1

        with open(os.path.join(root, 'info/synsets.txt')) as f:
            lines = f.readlines()

        self.txtlabels = {}

        for linenum, line in enumerate(lines):
            nxxxxxxxx = line.split()[0]
            if nxxxxxxxx in synset_set:
                target = idxs_cls[nxxxxxxxx]
                clstxtlabel = synset2txtlabel[nxxxxxxxx]  # clipstyle txtlabel
                self.txtlabels[target] = clstxtlabel

    def __getitem__(self, index):
        img_path = self.img_path[index]
        target = self.targets[index]
        image = Image.open(img_path).convert('RGB')
        img = self.transform(image)
        return img, target

    def __len__(self):
        return len(self.targets)


class ImageNet1K(Dataset):
    def __init__(self, datadir, transform, label_file, synset2txtlabel, split='train', **kwargs):
        super().__init__()
        self.datadir = datadir
        self.label_file = label_file
        self.img_datadir = os.path.join(datadir, 'ILSVRC/Data/CLS-LOC', split)
        self.transform = transform
        self.synset2txtlabel = synset2txtlabel
        self.txtlabels = {}

        with open(os.path.join(self.datadir, self.label_file), 'r') as f:
            self.synset2target = json.load(f)

        if split == 'train':
            self._init_trainsplit()
        elif split == 'val':
            self._init_valsplit()

    def _init_trainsplit(self):
        synsets = os.listdir(self.img_datadir)
        synsets.sort()

        self.img_path = []
        self.targets = []

        for synset in synsets:
            if synset not in self.synset2target.keys():
                continue
            target = self.synset2target[synset]
            self.txtlabels[target] = self.synset2txtlabel[synset]

            synset_dir = os.path.join(self.img_datadir, synset)
            imgpath_c = sorted(os.listdir(synset_dir))
            imgpath_c = [os.path.join(synset_dir, img) for img in imgpath_c]

            for imgpath_c_i in imgpath_c:
                self.img_path.append(imgpath_c_i)
                self.targets.append(target)

    def _init_valsplit(self):
        self.img_path = [os.path.join(self.img_datadir, img) for img in os.listdir(self.img_datadir)]
        self.img_path.sort()
        with open(os.path.join(self.datadir, 'ILSVRC/Annotations/CLS-LOC/val/imagenet_2012_validation_synset_labels.txt'), 'r') as f:
            synsets = list(f.read().splitlines())
        self.targets = [self.synset2target[t] for t in synsets]
        for synset in synsets:
            target = self.synset2target[synset]
            self.txtlabels[target] = self.synset2txtlabel[synset]

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, index):
        path = self.img_path[index]
        label = self.targets[index]

        with open(path, 'rb') as f:
            sample = Image.open(f).convert('RGB')

        if self.transform is not None:
            sample = self.transform(sample)

        return sample, label

